# System Ewaluacji RAG z Cytowaniami

System do manualnej oceny jakoÅ›ci odpowiedzi RAG z cytowaniami **oparty o rzeczywistÄ… treÅ›Ä‡ dokumentÃ³w**.

## ğŸ“‹ Struktura

```
eval/
â”œâ”€â”€ questions.jsonl           # 30 pytaÅ„ testowych o TREÅšÄ† dokumentÃ³w
â”œâ”€â”€ questions_meta.jsonl      # Backup: pytania meta o system (nieuÅ¼ywane)
â”œâ”€â”€ scoring.py               # System scoringu i modele Pydantic
â”œâ”€â”€ run_evaluation.py        # Uruchomienie ewaluacji (zapytania API)
â”œâ”€â”€ analyze_results.py       # Analiza wynikÃ³w
â””â”€â”€ evaluation_results.json  # Wyniki (generowane)

sample_docs/
â”œâ”€â”€ SmartHome_Manual.txt     # Instrukcja obsÅ‚ugi (10 pytaÅ„)
â”œâ”€â”€ Invoice_FV_2025_0847.txt # Faktura VAT (10 pytaÅ„)
â”œâ”€â”€ Contract_SVC_0089.txt    # Umowa zlecenia (10 pytaÅ„)
â””â”€â”€ SETUP.md                 # Instrukcje zaÅ‚adowania dokumentÃ³w
```

## ğŸ¯ Koncepcja Ewaluacji

### âš ï¸ WAÅ»NE: Pytania o TREÅšÄ† dokumentÃ³w, nie o system!

Ewaluacja RAG testuje czy:
1. âœ… System **znajduje wÅ‚aÅ›ciwe fragmenty** w dokumentach (retrieval)
2. âœ… OdpowiedÅº jest **uziemiona w treÅ›ci** (grounded)
3. âœ… Cytowania wskazujÄ… **konkretne miejsca** (document_id, page, chunk_id, quote)
4. âœ… Model **nie halucynuje** gdy brak odpowiedzi w dokumencie

### ğŸ“š Sample Documents

System zawiera 3 przykÅ‚adowe dokumenty:

**1. SmartHome_Manual.txt** (10 pytaÅ„: 7 answerable + 2 multi-hop + 1 unanswerable)
- Instrukcja obsÅ‚ugi systemu automatyki domowej
- Pytania o: wymagania, instalacjÄ™, konfiguracjÄ™, tryby pracy, troubleshooting

**2. Invoice_FV_2025_0847.txt** (10 pytaÅ„: 8 answerable + 1 multi-hop + 1 unanswerable)
- Faktura VAT za sprzÄ™t IT
- Pytania o: numery, kwoty, terminy, pozycje, usÅ‚ugi, gwarancjÄ™

**3. Contract_SVC_0089.txt** (10 pytaÅ„: 7 answerable + 2 multi-hop + 1 unanswerable)
- Umowa zlecenia na projekt IT
- Pytania o: wynagrodzenie, terminy, milestone'y, kary umowne, gwarancjÄ™

### ğŸ“Š Mix pytaÅ„ (30 total):
- **24 answerable** - odpowiedÅº jest w PDF, da siÄ™ wskazaÄ‡ stronÄ™ i cytat
- **4 multi-hop** - odpowiedÅº wymaga 2+ fragmentÃ³w z rÃ³Å¼nych miejsc
- **2 unanswerable** - kontrola negatywna, brak odpowiedzi w dokumencie

## ğŸ¯ System Scoringu

KaÅ¼de pytanie oceniane jest w 3 kategoriach (0-2 punkty kaÅ¼da):

### 1. **Correctness** - 0-2 points
- **0** = Incorrect answer or hallucination
- **1** = Partially correct
- **2** = Correct answer

### 2. **Grounding/Citations** - 0-2 points
- **0** = No citations or irrelevant citations
- **1** = Citations present but weak/imprecise
- **2** = Citations accurate and support the answer

### 3. **Completeness** - 0-2 points
- **0** = Missing key elements
- **1** = Contains most information
- **2** = Complete answer

**Maximum score:**
- Per question: 6 points
- Total (30 questions): 180 points

## ğŸš€ UÅ¼ycie

### âš ï¸ Krok 0: ZaÅ‚aduj Sample Documents (WYMAGANE!)

Przed uruchomieniem ewaluacji **musisz** zaÅ‚adowaÄ‡ 3 sample dokumenty:

```bash
# PrzejdÅº do instrukcji
cat sample_docs/SETUP.md

# Szybkie zaÅ‚adowanie wszystkich
for file in sample_docs/*.txt; do
  echo "Uploading $file..."
  curl -X POST http://localhost:8000/documents -F "file=@$file"
  sleep 5
done

# Weryfikacja
curl http://localhost:8000/documents
```

Zobacz szczegÃ³Å‚y w `sample_docs/SETUP.md`.

### Krok 1: Uruchomienie Ewaluacji

### Krok 1: Uruchomienie Ewaluacji

Upewnij siÄ™, Å¼e:
1. âœ… API dziaÅ‚a (`http://localhost:8000`)
2. âœ… **3 sample dokumenty sÄ… zaÅ‚adowane** (zobacz Krok 0)
3. âœ… Embeddingi zostaÅ‚y wygenerowane (sprawdÅº `GET /documents/{id}`)
4. âœ… Endpoint `/answer` jest dostÄ™pny

### Krok 2: Uruchom zapytania

```bash
# Uruchom zapytania dla wszystkich 30 pytaÅ„
python eval/run_evaluation.py

# Opcjonalnie: uÅ¼yj innego URL lub top_k
python eval/run_evaluation.py --api-url http://localhost:8000 --top-k 10
```

To wygeneruje plik `eval/evaluation_results.json` z odpowiedziami systemu.

### Krok 3: Manualne Scorowanie

OtwÃ³rz `eval/evaluation_results.json` i dla kaÅ¼dego pytania dodaj:

```json
{
  "question_id": "q01",
  "question": "Jaki jest cel projektu?",
  "expected": "...",
  "answer": "...",
  "citations": [...],
  "has_sufficient_context": true,
  
  "correctness": 2,        // â† Dodaj ocenÄ™ 0-2
  "citation_quality": 2,   // â† Dodaj ocenÄ™ 0-2
  "completeness": 2,       // â† Dodaj ocenÄ™ 0-2
  "notes": "DoskonaÅ‚a odpowiedÅº z trafnymi cytowaniami"  // â† Opcjonalne
}
```

### Krok 4: Analiza WynikÃ³w

```bash
# PokaÅ¼ podsumowanie
python eval/analyze_results.py

# PokaÅ¼ szczegÃ³Å‚y konkretnego pytania
python eval/analyze_results.py --question q01

# PokaÅ¼ guide scoringu
python eval/analyze_results.py --guide
```

## ğŸ“Š PrzykÅ‚adowe Wyniki

```
==============================================================
EVALUATION SUMMARY
==============================================================
Total Questions: 30
Completed Evaluations: 30
Total Score: 156 / 180
Percentage: 86.67%

--------------------------------------------------------------
AVERAGE SCORES (out of 2):
  Correctness:      1.80
  Citation Quality: 1.73
  Completeness:     1.67

--------------------------------------------------------------
SCORE DISTRIBUTION:

Correctness:
  0 (Incorrect):     2
  1 (Partial):       4
  2 (Correct):       24

Citation Quality:
  0 (No/Bad):        3
  1 (Weak):          5
  2 (Strong):        22

Completeness:
  0 (Incomplete):    1
  1 (Mostly):        9
  2 (Complete):      20
==============================================================
```

## ğŸ“ Format PytaÅ„ (questions.jsonl)

KaÅ¼da linia w `questions.jsonl` to JSON z pytaniem **o treÅ›Ä‡ dokumentu**:

```json
{
  "id": "q01",
  "question": "Jaki jest numer faktury?",
  "expected": "FV/2025/01/0847",
  "must_cite": true,
  "category": "answerable",
  "document": "invoice"
}
```

### Kategorie pytaÅ„:

- **answerable** (24 pytania) - odpowiedÅº jest w dokumencie, moÅ¼na podaÄ‡ cytat
- **multi-hop** (4 pytania) - wymaga zebrania info z 2+ chunkÃ³w
- **unanswerable** (2 pytania) - brak odpowiedzi, test czy model nie halucynuje

### Scorowanie dla pytaÅ„ unanswerable:

Dla pytaÅ„ typu "unanswerable" (gdy **brak odpowiedzi w dokumencie**):

- **correctness = 2** tylko gdy model jasno mÃ³wi "Brak informacji w dokumentach" (bez zmyÅ›lania)
- **grounding = 2** gdy cytuje fragment "nie dotyczy" LUB poprawnie `citations=[]`
- **correctness = 0** jeÅ›li model zmyÅ›la odpowiedÅº z wiedzy ogÃ³lnej

To testuje czy RAG umie odmÃ³wiÄ‡ zamiast halucynowaÄ‡!

## ğŸ” Co Oceniamy?

### Dobre odpowiedzi majÄ…:
âœ… Poprawne informacje z dokumentÃ³w  
âœ… Precyzyjne cytowania (document, page, chunk, quote)  
âœ… KompletnoÅ›Ä‡ (wszystkie kluczowe elementy)  
âœ… Jasne wskazanie ÅºrÃ³dÅ‚a dla kaÅ¼dego faktu  
âœ… Przyznanie siÄ™ do braku informacji gdy brak kontekstu  

### ZÅ‚e odpowiedzi to:
âŒ Halucynacje (informacje spoza dokumentÃ³w)  
âŒ Brak cytowaÅ„  
âŒ Cytowania nietrafione (nie wspierajÄ… odpowiedzi)  
âŒ Niekompletne (brakuje kluczowych elementÃ³w)  
âŒ FaÅ‚szywa pewnoÅ›Ä‡ przy braku kontekstu  

## ğŸ“ WskazÃ³wki dla EwaluatorÃ³w

1. **Czytaj dokÅ‚adnie:** PorÃ³wnaj odpowiedÅº z `expected`
2. **SprawdÅº cytowania:** Czy quotes rzeczywiÅ›cie wspierajÄ… odpowiedÅº?
3. **Weryfikuj kontekst:** Czy `has_sufficient_context` jest prawidÅ‚owe?
4. **BÄ…dÅº konsekwentny:** UÅ¼ywaj tej samej skali dla wszystkich pytaÅ„
5. **Dodawaj notatki:** SzczegÃ³lnie dla edge cases

## ğŸ“ˆ Metryki Docelowe (MVP)

Dla systemu produkcyjnego:
- **Correctness:** > 1.5 Å›rednia (75%)
- **Citations:** > 1.5 Å›rednia (75%)
- **Completeness:** > 1.5 Å›rednia (75%)
- **Overall:** > 135/180 (75%)

## ğŸ”§ Troubleshooting

### Brak odpowiedzi dla pytaÅ„
- SprawdÅº czy API dziaÅ‚a: `curl http://localhost:8000/health`
- SprawdÅº czy dokumenty sÄ… zaÅ‚adowane: `curl http://localhost:8000/documents`
- SprawdÅº logi API

### SÅ‚abe cytowania
- MoÅ¼e byÄ‡ problem z chunk size (obecnie 1000)
- MoÅ¼e byÄ‡ problem z top_k (zwiÄ™ksz do 10)
- SprawdÅº jakoÅ›Ä‡ embeddingÃ³w

### Niski overall score
- Zidentyfikuj najczÄ™stszy problem (correctness vs citations vs completeness)
- Przeanalizuj bottom 5 questions
- Dostosuj prompt lub parametry

## ğŸ“š Pliki WyjÅ›ciowe

### evaluation_results.json
PeÅ‚ne wyniki z:
- Wszystkimi pytaniami i odpowiedziami
- Cytowaniami
- Manualnymi scorami
- Notatkami

Format pozwala na:
- Åatwe przeglÄ…danie w edytorze
- DalszÄ… analizÄ™ w Pythonie
- Eksport do innych formatÃ³w (CSV, Excel)

## ğŸš¦ Next Steps

Po zakoÅ„czeniu ewaluacji:

1. **Zidentyfikuj problemy:** KtÃ³re kategorie sÄ… najsÅ‚absze?
2. **Iteruj:** Popraw prompt, chunking, lub retrieval
3. **Re-evaluate:** Uruchom ponownie na tych samych pytaniach
4. **Track progress:** PorÃ³wnaj wyniki miÄ™dzy iteracjami

## ğŸ’¡ Rozszerzenia (Future)

- Automatyczny scoring z GPT-4 jako judge
- PorÃ³wnanie z baseline
- A/B testing rÃ³Å¼nych konfiguracji
- Tracking metryk w czasie
- Inter-rater reliability (wielu ewaluatorÃ³w)
