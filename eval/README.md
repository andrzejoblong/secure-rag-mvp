# System Ewaluacji RAG z Cytowaniami

System do manualnej oceny jakoÅ›ci odpowiedzi RAG z cytowaniami.

## ğŸ“‹ Struktura

```
eval/
â”œâ”€â”€ questions.jsonl           # 30 pytaÅ„ testowych
â”œâ”€â”€ scoring.py               # System scoringu i modele Pydantic
â”œâ”€â”€ run_evaluation.py        # Uruchomienie ewaluacji (zapytania API)
â”œâ”€â”€ analyze_results.py       # Analiza wynikÃ³w
â””â”€â”€ evaluation_results.json  # Wyniki (generowane)
```

## ğŸ¯ System Scoringu

KaÅ¼de pytanie oceniane jest w 3 kategoriach (0-2 punkty kaÅ¼da):

### 1. **Correctness (PoprawnoÅ›Ä‡)** - 0-2 punkty
- **0** = BÅ‚Ä™dna odpowiedÅº lub halucynacja
- **1** = CzÄ™Å›ciowo poprawna
- **2** = Poprawna odpowiedÅº

### 2. **Grounding/Citations (Cytowania)** - 0-2 punkty
- **0** = Brak cytowaÅ„ lub cytowania nietrafione
- **1** = SÄ… cytowania, ale sÅ‚abe/nieprecyzyjne
- **2** = Cytowania trafne i wspierajÄ… odpowiedÅº

### 3. **Completeness (KompletnoÅ›Ä‡)** - 0-2 punkty
- **0** = Pomija kluczowe elementy
- **1** = Zawiera wiÄ™kszoÅ›Ä‡ informacji
- **2** = Kompletna odpowiedÅº

**Maksymalny wynik:**
- Na pytanie: 6 punktÃ³w
- ÅÄ…cznie (30 pytaÅ„): 180 punktÃ³w

## ğŸš€ UÅ¼ycie

### Krok 1: Przygotowanie

Upewnij siÄ™, Å¼e:
1. API dziaÅ‚a (`http://localhost:8000`)
2. Dokumenty sÄ… zaÅ‚adowane i przetworzone
3. Endpoint `/answer` jest dostÄ™pny

### Krok 2: Uruchomienie Ewaluacji

```bash
# Uruchom zapytania dla wszystkich 30 pytaÅ„
python eval/run_evaluation.py

# Opcjonalnie: uÅ¼yj innego URL lub top_k
python eval/run_evaluation.py --api-url http://localhost:8000 --top-k 10
```

To wygeneruje plik `eval/evaluation_results.json` z odpowiedziami systemu.

### Krok 3: Manualne Scorowanie

OtwÃ³rz `eval/evaluation_results.json` i dla kaÅ¼dego pytania dodaj:

```json
{
  "question_id": "q01",
  "question": "Jaki jest cel projektu?",
  "expected": "...",
  "answer": "...",
  "citations": [...],
  "has_sufficient_context": true,
  
  "correctness": 2,        // â† Dodaj ocenÄ™ 0-2
  "citation_quality": 2,   // â† Dodaj ocenÄ™ 0-2
  "completeness": 2,       // â† Dodaj ocenÄ™ 0-2
  "notes": "DoskonaÅ‚a odpowiedÅº z trafnymi cytowaniami"  // â† Opcjonalne
}
```

### Krok 4: Analiza WynikÃ³w

```bash
# PokaÅ¼ podsumowanie
python eval/analyze_results.py

# PokaÅ¼ szczegÃ³Å‚y konkretnego pytania
python eval/analyze_results.py --question q01

# PokaÅ¼ guide scoringu
python eval/analyze_results.py --guide
```

## ğŸ“Š PrzykÅ‚adowe Wyniki

```
==============================================================
EVALUATION SUMMARY
==============================================================
Total Questions: 30
Completed Evaluations: 30
Total Score: 156 / 180
Percentage: 86.67%

--------------------------------------------------------------
AVERAGE SCORES (out of 2):
  Correctness:      1.80
  Citation Quality: 1.73
  Completeness:     1.67

--------------------------------------------------------------
SCORE DISTRIBUTION:

Correctness:
  0 (Incorrect):     2
  1 (Partial):       4
  2 (Correct):       24

Citation Quality:
  0 (No/Bad):        3
  1 (Weak):          5
  2 (Strong):        22

Completeness:
  0 (Incomplete):    1
  1 (Mostly):        9
  2 (Complete):      20
==============================================================
```

## ğŸ“ Format PytaÅ„ (questions.jsonl)

KaÅ¼da linia w `questions.jsonl` to JSON z pytaniem:

```json
{
  "id": "q01",
  "question": "Jaki jest cel projektu?",
  "expected": "Oczekiwana odpowiedÅº lub kluczowe elementy",
  "must_cite": true
}
```

## ğŸ” Co Oceniamy?

### Dobre odpowiedzi majÄ…:
âœ… Poprawne informacje z dokumentÃ³w  
âœ… Precyzyjne cytowania (document, page, chunk, quote)  
âœ… KompletnoÅ›Ä‡ (wszystkie kluczowe elementy)  
âœ… Jasne wskazanie ÅºrÃ³dÅ‚a dla kaÅ¼dego faktu  
âœ… Przyznanie siÄ™ do braku informacji gdy brak kontekstu  

### ZÅ‚e odpowiedzi to:
âŒ Halucynacje (informacje spoza dokumentÃ³w)  
âŒ Brak cytowaÅ„  
âŒ Cytowania nietrafione (nie wspierajÄ… odpowiedzi)  
âŒ Niekompletne (brakuje kluczowych elementÃ³w)  
âŒ FaÅ‚szywa pewnoÅ›Ä‡ przy braku kontekstu  

## ğŸ“ WskazÃ³wki dla EwaluatorÃ³w

1. **Czytaj dokÅ‚adnie:** PorÃ³wnaj odpowiedÅº z `expected`
2. **SprawdÅº cytowania:** Czy quotes rzeczywiÅ›cie wspierajÄ… odpowiedÅº?
3. **Weryfikuj kontekst:** Czy `has_sufficient_context` jest prawidÅ‚owe?
4. **BÄ…dÅº konsekwentny:** UÅ¼ywaj tej samej skali dla wszystkich pytaÅ„
5. **Dodawaj notatki:** SzczegÃ³lnie dla edge cases

## ğŸ“ˆ Metryki Docelowe (MVP)

Dla systemu produkcyjnego:
- **Correctness:** > 1.5 Å›rednia (75%)
- **Citations:** > 1.5 Å›rednia (75%)
- **Completeness:** > 1.5 Å›rednia (75%)
- **Overall:** > 135/180 (75%)

## ğŸ”§ Troubleshooting

### Brak odpowiedzi dla pytaÅ„
- SprawdÅº czy API dziaÅ‚a: `curl http://localhost:8000/health`
- SprawdÅº czy dokumenty sÄ… zaÅ‚adowane: `curl http://localhost:8000/documents`
- SprawdÅº logi API

### SÅ‚abe cytowania
- MoÅ¼e byÄ‡ problem z chunk size (obecnie 1000)
- MoÅ¼e byÄ‡ problem z top_k (zwiÄ™ksz do 10)
- SprawdÅº jakoÅ›Ä‡ embeddingÃ³w

### Niski overall score
- Zidentyfikuj najczÄ™stszy problem (correctness vs citations vs completeness)
- Przeanalizuj bottom 5 questions
- Dostosuj prompt lub parametry

## ğŸ“š Pliki WyjÅ›ciowe

### evaluation_results.json
PeÅ‚ne wyniki z:
- Wszystkimi pytaniami i odpowiedziami
- Cytowaniami
- Manualnymi scorami
- Notatkami

Format pozwala na:
- Åatwe przeglÄ…danie w edytorze
- DalszÄ… analizÄ™ w Pythonie
- Eksport do innych formatÃ³w (CSV, Excel)

## ğŸš¦ Next Steps

Po zakoÅ„czeniu ewaluacji:

1. **Zidentyfikuj problemy:** KtÃ³re kategorie sÄ… najsÅ‚absze?
2. **Iteruj:** Popraw prompt, chunking, lub retrieval
3. **Re-evaluate:** Uruchom ponownie na tych samych pytaniach
4. **Track progress:** PorÃ³wnaj wyniki miÄ™dzy iteracjami

## ğŸ’¡ Rozszerzenia (Future)

- Automatyczny scoring z GPT-4 jako judge
- PorÃ³wnanie z baseline
- A/B testing rÃ³Å¼nych konfiguracji
- Tracking metryk w czasie
- Inter-rater reliability (wielu ewaluatorÃ³w)
