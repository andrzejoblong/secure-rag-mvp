# RAG Implementation - Portfolio Project

**Candidate**: [Your Name]  
**Project**: Production-Ready RAG System with Hybrid Search  
**Repository**: https://github.com/andrzejoblong/secure-rag-mvp  
**Date**: February 2026

---

## ðŸ“‹ Executive Summary

Built a **production-ready Retrieval-Augmented Generation (RAG) system from scratch** to demonstrate deep understanding of AI/ML backend engineering. Project achieves **83% context accuracy** and **70% citation accuracy** through custom hybrid search algorithm.

**Key Achievement**: Custom BM25 + semantic hybrid search improved exact match retrieval by **161%** compared to pure semantic search.

---

## ðŸŽ¯ Technical Highlights

### What Was Built

1. **Custom RAG Architecture** (not framework-based)
   - Designed and implemented from scratch
   - Full control over chunking, retrieval, and generation
   - Optimized for accuracy and cost

2. **Hybrid Search Algorithm**
   - Combined BM25 (keyword) + semantic (embeddings)
   - 30% BM25 weight + 70% semantic weight
   - Score normalization and fusion
   - **Result**: +25% better context retrieval

3. **LLM Integration (OpenAI GPT-4o-mini)**
   - Structured JSON outputs with Pydantic validation
   - Prompt engineering for grounded responses
   - Citation extraction (document, page, chunk, quote)
   - Fallback mechanisms (local embeddings â†’ OpenAI)

4. **Evaluation Framework**
   - 30-question benchmark across 3 document types
   - 3-metric scoring (correctness, citations, completeness)
   - Quantitative measurement of improvements
   - Manual scoring workflow

### Technology Stack

**Backend**: FastAPI, SQLAlchemy, Alembic migrations  
**Database**: PostgreSQL 15 + pgvector (vector search)  
**Embeddings**: sentence-transformers (local) + OpenAI (fallback)  
**Search**: BM25Okapi (rank-bm25) + semantic similarity  
**LLM**: OpenAI GPT-4o-mini with structured outputs  
**Infrastructure**: Docker Compose, Poetry, pytest

---

## ðŸ“Š Results & Metrics

### Quantitative Improvements

| Metric | Before (Semantic Only) | After (Hybrid) | Improvement |
|--------|----------------------|----------------|-------------|
| Context Accuracy | 67% (20/30) | **83% (25/30)** | +25% |
| Citation Accuracy | 60% (18/30) | **70% (21/30)** | +17% |
| Exact Match Score | 0.338 | **0.885** | +161% |

### Example Query

**Query**: "FV/2025/01/0847" (invoice number lookup)

**Before (Semantic)**:
- Relevance: 33.8%
- Result: Incorrect chunk retrieved

**After (Hybrid)**:
- Relevance: 88.5%
- Result: âœ… Correct invoice chunk ranked #1
- Answer: "Invoice number is FV/2025/01/0847" with citation

---

## ðŸ’¡ Engineering Decisions

### 1. Why Build From Scratch vs. Using LangChain?

**Decision**: Custom implementation first

**Reasoning**:
- Deep understanding of RAG components
- Full control over chunking strategy (tested 1000 â†’ 2000 chars)
- Optimized hybrid search (BM25 + semantic)
- No framework overhead or lock-in
- Easy to debug (know every layer)

**Can add LangChain**: Integration is trivial (~2-3 hours) on this foundation

### 2. Why Hybrid Search?

**Problem**: Pure semantic search failed on exact matches
- Query: "FV/2025/01/0847" â†’ 33.8% relevance âŒ
- Struggled with invoice numbers, IDs, technical specs

**Solution**: BM25 (keyword) + semantic (meaning)
- Query: "FV/2025/01/0847" â†’ 88.5% relevance âœ…
- Best of both worlds: exact matches + semantic understanding

### 3. Why Custom Evaluation?

**Need**: Quantitative measurement of RAG quality

**Solution**: 30-question benchmark
- 3 document types (invoice, manual, contract)
- 3 metrics per question (correctness, citations, completeness)
- Manual scoring workflow
- Track improvements over iterations

**Result**: Data-driven optimization (know what works)

---

## ðŸŽ“ Skills Demonstrated

### RAG Architecture & Design
- âœ… Custom chunking strategies (experimentation with sizes/overlaps)
- âœ… Hybrid search algorithm (BM25 + semantic fusion)
- âœ… Citation extraction and source tracking
- âœ… Evaluation methodology (quantitative measurement)

### LLM Production Integration
- âœ… OpenAI API with error handling and retries
- âœ… Prompt engineering for grounded responses
- âœ… Structured JSON outputs with validation
- âœ… Cost optimization (local embeddings first, OpenAI fallback)
- âœ… Token usage awareness

### Backend Engineering
- âœ… FastAPI with OpenAPI documentation
- âœ… PostgreSQL + pgvector for vector search
- âœ… SQLAlchemy ORM with Alembic migrations
- âœ… Background task processing
- âœ… Environment variable management
- âœ… Docker containerization

### Testing & Quality
- âœ… Comprehensive evaluation suite (30 questions)
- âœ… Manual scoring workflow (0-180 point scale)
- âœ… Quantitative improvement tracking
- âœ… Sample document preparation

---

## ðŸš€ Production Readiness

### What's Production-Ready Now
âœ… Error handling and fallbacks  
âœ… Structured outputs with validation  
âœ… Environment variable configuration  
âœ… Database migrations (Alembic)  
âœ… Docker containerization  
âœ… Cost optimization (local embeddings)  
âœ… Evaluation framework

### What's Missing (Can Add)
âš ï¸ Kubernetes deployment (4-6 hours)  
âš ï¸ Monitoring/observability (2-3 hours)  
âš ï¸ Streaming responses (2-3 hours)  
âš ï¸ CI/CD pipeline (2-3 hours)

**Estimated time to full production**: 10-15 hours

---

## ðŸ“ˆ Future Enhancements (Roadmap)

### Phase 1: LangChain Integration (3-4h)
- Replace custom generation with LangChain RetrievalQA
- Add LangGraph for multi-step reasoning
- Tool calling for external data sources

### Phase 2: Streaming & Async (2-3h)
- Server-Sent Events (SSE) for real-time tokens
- MCP server pattern implementation
- Progress updates during retrieval/generation

### Phase 3: AWS/K8s Deployment (4-6h)
- Kubernetes manifests (deployment, service, ingress)
- AWS EKS cluster setup
- CloudWatch monitoring + auto-scaling
- CI/CD pipeline (GitHub Actions)

### Phase 4: Agentic Features (6-8h)
- Multi-step reasoning with LangGraph
- Self-critique and refinement loops
- Dynamic tool selection

**Total time to 95%+ role match**: ~15-20 hours

---

## ðŸ’¼ Alignment with Role Requirements

**Role**: AI/ML Backend Engineer with RAG expertise

### âœ… Strong Match (85%)

1. **"5+ years Python, AI/ML/backend"**
   - âœ… Production FastAPI application
   - âœ… SQLAlchemy ORM, Alembic migrations
   - âœ… Async processing, background tasks

2. **"RAG design and implementation"**
   - âœ… Built from scratch (not just framework)
   - âœ… Custom hybrid search algorithm
   - âœ… Citation extraction with sources
   - âœ… Evaluation framework

3. **"LLM production experience"**
   - âœ… OpenAI integration with error handling
   - âœ… Prompt engineering for grounding
   - âœ… Structured outputs, cost optimization
   - âœ… Fallback mechanisms

4. **"Vector databases"**
   - âœ… pgvector with 384-dim vectors
   - âœ… Semantic search, cosine similarity
   - âœ… Hybrid ranking algorithms

5. **"Secure coding, data protection"**
   - âœ… Environment variables for secrets
   - âœ… SQL injection prevention (parameterized queries)
   - âœ… Input validation (Pydantic)

### âš ï¸ Can Add (Gap Closers)

6. **"LangChain/LangGraph frameworks"**
   - âš ï¸ Deliberately not used (to show custom skills)
   - âœ… Can integrate in 2-4 hours

7. **"MCP servers, streaming, async"**
   - âš ï¸ Current: synchronous REST API
   - âœ… Can add SSE/streaming in 2-3 hours

8. **"AWS, K8s, production deployment"**
   - âš ï¸ Current: Docker Compose (local dev)
   - âœ… Can deploy to EKS in 4-6 hours

**Current Match**: 85% â†’ **Target Match**: 95%+ (with 10-15h extensions)

---

## ðŸ“ Code Samples

### 1. Hybrid Search Implementation

```python
class HybridSearcher:
    def __init__(self, chunks, bm25_weight=0.3, semantic_weight=0.7):
        self.corpus = [self._tokenize(c['text']) for c in chunks]
        self.bm25 = BM25Okapi(self.corpus)
        
    def search(self, query, semantic_scores, top_k=10):
        # Get BM25 scores
        bm25_scores = self.bm25.get_scores(self._tokenize(query))
        
        # Normalize both to [0,1]
        bm25_norm = self._normalize_scores(bm25_scores)
        semantic_norm = self._normalize_scores(semantic_scores)
        
        # Weighted combination
        combined = 0.3 * bm25_norm + 0.7 * semantic_norm
        
        return sorted(enumerate(combined), reverse=True)[:top_k]
```

### 2. Grounded Answer Generation

```python
SYSTEM_PROMPT = """You are an assistant that answers ONLY based on context.

RULES:
1. Answer ONLY from provided context
2. If no info: say "No information in documents"
3. Provide citations for each fact
4. DO NOT add external knowledge

FORMAT: JSON with {answer, citations, has_sufficient_context}
"""
```

### 3. Evaluation Framework

```python
class QuestionEvaluation(BaseModel):
    correctness: int = Field(ge=0, le=2)
    citation_quality: int = Field(ge=0, le=2)
    completeness: int = Field(ge=0, le=2)
    
    @property
    def total_score(self) -> int:
        return self.correctness + self.citation_quality + self.completeness

# 30 questions Ã— 6 points = 180 max
```

---

## ðŸŽ¯ Why This Project Matters

### Demonstrates Real-World Skills

1. **Problem-Solving**: Identified semantic search weakness â†’ designed hybrid solution
2. **Data-Driven**: Built evaluation framework to measure improvements quantitatively
3. **Production Mindset**: Error handling, fallbacks, cost optimization, migrations
4. **Quality Focus**: 30-question benchmark, manual scoring, iterative improvement

### Shows Learning & Adaptation

- Experimented with chunk sizes (1000 â†’ 2000 chars)
- Tested pure semantic â†’ recognized limitations â†’ implemented hybrid
- Built evaluation framework to validate changes
- Documented decisions and tradeoffs

### Practical Results

- **83% context accuracy** (up from 67%)
- **70% citation accuracy** (up from 60%)
- **161% improvement** in exact match queries
- **Quantitative evidence** of engineering quality

---

## ðŸ“ž Contact & Links

**GitHub**: https://github.com/andrzejoblong/secure-rag-mvp  
**Documentation**: See `README.md` and `SESSION_NOTES.md` in repository  
**Live Demo**: Can provide on request (requires Docker + OpenAI API key)

**Ready to discuss**:
- Architecture decisions and tradeoffs
- Hybrid search algorithm design
- Production deployment strategies
- Extension to LangChain/LangGraph
- Streaming and async patterns
- AWS/K8s deployment approach

---

**Project Status**: âœ… Core complete, evaluation proven, ready for extensions  
**Time Investment**: ~8 hours (core RAG) + ~3 hours (hybrid search & evaluation)  
**Demonstrates**: 85% role match with clear 10-15h path to 95%+
